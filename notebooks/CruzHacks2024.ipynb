{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7560cca5-0095-4fa1-8095-cc5e6b35a442",
   "metadata": {},
   "source": [
    "# Create AI apps with LLMs\n",
    "\n",
    "This notebook shows ways to experiment quickly with Large Language Models (LLMs) and \n",
    "Retrieval Augmented Generation (RAG) which can then be integrated into a UI / App\n",
    "\n",
    "For putting things together, Langchain is a very useful framework that integrates lots of different providers \n",
    "(LLM, vector databases, agents...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f93bd-f495-4f2e-b0a5-3fbcbca0134e",
   "metadata": {},
   "source": [
    "## RAG: Retrieval Augmented Generation\n",
    "\n",
    "LLMs only know about things they were trained on. They cannot know about everything, especially not \n",
    "about documents and data from private sources, or content published after the model was trained.\n",
    "\n",
    "To generate accurate answers querying specific content, the content needs to be passed to the LLM as part of the prompt.\n",
    "However, there is a major problem: despite having a large context window compared to other types of NLP models, the window \n",
    "is not unlimited. Typical window size is 1024, 4096, and up to 32000 tokens, which is often too small for even medium size documents.\n",
    "                                                                                                     \n",
    "The solution is to index the content, and provide only relevant context to the LLM.\n",
    "\n",
    "To do this, the content is chunked into small pieces of text, for each piece, an embedding vector of the sentence is created, and stored into\n",
    "a vector store. Upon querying the data, an embedding of the query is created, and the vector store is queried for similar content.\n",
    "The top N pieces of relevant content are retrieved and plugged into the prompt for the LLM to answer the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406b850-3d85-461c-823d-430bcdb8efb6",
   "metadata": {},
   "source": [
    "### Import some data to query\n",
    "\n",
    "In this example, we retrieve a recent article from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d635ec2-fde0-4fb3-b87e-8b379831d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# for other types of documents, use:\n",
    "# from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "# from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "# from langchain_community.document_loaders import JSONLoader\n",
    "# from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# see: https://python.langchain.com/docs/modules/data_connection/document_loaders/ for more info\n",
    "\n",
    "url = \"https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends\"\n",
    "loader = WebBaseLoader(url)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2542ea-be32-4576-b665-8fb4372b7f2b",
   "metadata": {},
   "source": [
    "### Chunk the article into smaller manageable pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7a9a07-4ae2-4ae3-9b47-dda8f914dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=0, \n",
    "    length_function=len, \n",
    ")\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80820fa6-c0d0-43ce-8212-c28e90ea466b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebaeb3-b449-46c9-b33c-61c1a74461f4",
   "metadata": {},
   "source": [
    "### Load an embedding model\n",
    "\n",
    "Note the same embedding model needs to be used to embed the pieces of text from the article, and later the query to be answered for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a9b479-08fb-42de-8d92-ee2090a14ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# If you are on Mac M1/M2, enable the following environment variable\n",
    "# NotImplementedError: The operator 'aten::cumsum.out' is not currently implemented for the MPS device. \n",
    "# If you want this op to be added in priority during the prototype phase of this feature, \n",
    "# please comment on https://github.com/pytorch/pytorch/issues/77764. \n",
    "# As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` \n",
    "# to use the CPU as a fallback for this op. \n",
    "# WARNING: this will be slower than running natively on MPS.\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'mps'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embedding=HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c28a6a-087b-4168-9ce7-b698d51c6a84",
   "metadata": {},
   "source": [
    "### Ingest the data into the vector store\n",
    "\n",
    "Langchain vector store interface takes care of embedding each piece of text and store it in the DB.\n",
    "\n",
    "Here we use ChromaDB, a local vector database based on SQLLite. \n",
    "\n",
    "Note as we pass the split texts and the embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc44323-6bf9-4424-9ecb-b942582c8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=\"./\"  # if you want to persist the DB locally, and not have to reindex each time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29baebd4-704d-44d1-a5a1-8ee9eacfae76",
   "metadata": {},
   "source": [
    "### Test the vector store retrieval on some question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61402235-6f93-48d6-ba3f-0923a24f2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is RAG?\"\n",
    "docs = vectorstore.similarity_search_with_score(question, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55a6422d-230e-48d9-b26d-4df748d7ff4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='\"You can use RAG to go gather a ton of unstructured information, documents, etc., [and] feed it into a model without having to fine-tune or custom-train a model,\" Barrington said.\\nThese benefits are particularly enticing for enterprise applications where up-to-date factual knowledge is crucial. For example, businesses can use RAG with foundation models to create more efficient and informative chatbots and virtual assistants.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  0.8618913764255481),\n",
       " (Document(page_content='RAG blends text generation with information retrieval to enhance the accuracy and relevance of AI-generated content. It enables LLMs to access external information, helping them produce more accurate and contextually aware responses. Bypassing the need to store all knowledge directly in the LLM also reduces model size, which increases speed and lowers costs.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  1.3268553484893524),\n",
       " (Document(page_content='adoption.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  1.7623641094719744),\n",
       " (Document(page_content='Shadow AI is just one facet of the larger phenomenon of shadow IT.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  1.7782779174839414),\n",
       " (Document(page_content='Contributors\\nReprints\\nAnswers\\nDefinitions\\nE-Products\\nEvents\\nFeatures\\n\\n\\nGuides\\nOpinions\\nPhoto Stories\\nQuizzes\\nTips\\nTutorials\\nVideos\\n\\n\\n\\n\\nAll Rights Reserved, \\nCopyright 2018 - 2024, TechTarget\\n\\n\\nPrivacy Policy\\n\\n\\n\\nCookie Preferences \\n\\n\\n\\nCookie Preferences \\n\\n\\n\\nDo Not Sell or Share My Personal Information\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClose', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  1.778641962678)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ead8e-96b8-45d1-8a19-bea5034e2313",
   "metadata": {},
   "source": [
    "### Setup the LLM\n",
    "\n",
    "### To use LlamaCpp\n",
    "\n",
    "LlamaCpp lets you run a model as a local LLM engine so everything runs locally. \n",
    "I use a small version of the model (13B params) quantized to 4bit that takes a lot less space than the full model.\n",
    "\n",
    "However, even then it requires a decent Nvidia GPU or a M1/M2 Mac. Alternatively, you can use a service like OpenAI.\n",
    "\n",
    "Note that on Mac, it requires to compile with special flags. See the README for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "517296ca-24b3-4e1a-8cfd-7e65540b953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import LlamaCpp\n",
    "# model_file = '/Users/emmanuel/workspace/models/llms/llama-2-13b-chat.Q4_0.gguf'\n",
    "# # downloaded from https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF\n",
    "# # you may also try a smaller model:\n",
    "# # https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "# llm = LlamaCpp(\n",
    "#     model_path=model_file,\n",
    "#     n_ctx=4096, # context window\n",
    "#     #verbose=True,\n",
    "#     device='mps',\n",
    "#     # model_kwargs={'device':'mps'},\n",
    "#     n_gpu_layers=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3718dfb4-1b96-4b56-98a6-585057a3d22e",
   "metadata": {},
   "source": [
    "## To use OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39231576-ecbd-40f6-ad71-934a3a40d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee4d2ad7-0bed-4d4d-80a8-0ea9288b680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "# llm = OpenAI(openai_api_key=\"...\")\n",
    "llm = OpenAI(openai_api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1786e-cacc-4b29-a097-64ced8e2d851",
   "metadata": {},
   "source": [
    "## To use Ollama\n",
    "\n",
    "first setup Ollama (install the app, run it, this installs the command line\n",
    "\n",
    "Then run the server with \n",
    "```ollama run <model>```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51151df5-c633-4c5e-9520-e9dc945cef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.callbacks.manager import CallbackManager\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "# from langchain_community.llms import Ollama\n",
    "\n",
    "# llm = Ollama(\n",
    "#     model=\"llama2\",\n",
    "#     # callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90c654-0432-46b0-9fee-3c9c88213892",
   "metadata": {},
   "source": [
    "### Setup a retrieval chain\n",
    "\n",
    "Another useful langchain abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "513f9f1a-ab44-4e56-a1b0-907c12e134d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectorstore.as_retriever(search_kwargs={'k':15}, search_type=\"mmr\"),\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e9be088-09fd-4d28-9032-34110a1585a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuel/workspace/code/me/CruzHacks/CruzHacks2024/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is RAG?',\n",
       " 'result': ' RAG is a technique that blends text generation with information retrieval to enhance the accuracy and relevance of AI-generated content. It enables language and learning models (LLMs) to access external information, helping them produce more accurate and contextually aware responses. It also reduces model size and costs by bypassing the need to store all knowledge directly in the LLM. ',\n",
       " 'source_documents': [Document(page_content='\"You can use RAG to go gather a ton of unstructured information, documents, etc., [and] feed it into a model without having to fine-tune or custom-train a model,\" Barrington said.\\nThese benefits are particularly enticing for enterprise applications where up-to-date factual knowledge is crucial. For example, businesses can use RAG with foundation models to create more efficient and informative chatbots and virtual assistants.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='RAG blends text generation with information retrieval to enhance the accuracy and relevance of AI-generated content. It enables LLMs to access external information, helping them produce more accurate and contextually aware responses. Bypassing the need to store all knowledge directly in the LLM also reduces model size, which increases speed and lowers costs.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='adoption.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='Shadow AI is just one facet of the larger phenomenon of shadow IT.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='Contributors\\nReprints\\nAnswers\\nDefinitions\\nE-Products\\nEvents\\nFeatures\\n\\n\\nGuides\\nOpinions\\nPhoto Stories\\nQuizzes\\nTips\\nTutorials\\nVideos\\n\\n\\n\\n\\nAll Rights Reserved, \\nCopyright 2018 - 2024, TechTarget\\n\\n\\nPrivacy Policy\\n\\n\\n\\nCookie Preferences \\n\\n\\n\\nCookie Preferences \\n\\n\\n\\nDo Not Sell or Share My Personal Information\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClose', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content=\"Qlik's Kyndi acquisition targets unstructured data, GenAI\\nQlik's acquisition of Kyndi adds support for unstructured data that can be combined with structured data to inform GenAI models ...\\n\\n\\n\\nMicrosoft unveils new GenAI, data tools for retail industry\\nThe tech giant is adding industry-specific features in Data Fabric along with GenAI Copilots designed to better personalize ...\", metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='\"The interfaces of the world are multimodal,\" said Mark Chen, head of frontiers research at OpenAI, in a November 2023 presentation at the conference EmTech MIT. \"We want our models to see what we see and hear what we hear, and we want them to also generate content that appeals to more than one of our senses.\"', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content=\"as NIST's AI Risk Management Framework and the Federal Trade Commission's statement warning businesses against making false claims about their products' AI use.\", metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='\"If you have very loose use cases that are not clearly defined, that\\'s probably what\\'s going to hold you up the most,\" Crossan said.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content=\"Ascend.io, DBT integration targets cost control, efficiency\\nThe partnership aims to address the difficulty of efficiently and inexpensively deploying and managing data models developed in ...\\n\\n\\n\\nPinecone unveils serverless vector database, targets costs\\nThe vendor's new tool is designed to help customers control their cloud computing costs while developing and maintaining ...\", metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='reach.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='Building a customized model rather than using an off-the-shelf public tool often also improves privacy and security, as it gives organizations greater control over their data. Luke gave the example of building a model for Workday tasks that involve handling sensitive personal data, such as disability status and health history. \"Those aren\\'t things that we\\'re going to want to send out to a third party,\" he said. \"Our customers generally wouldn\\'t be comfortable with that.\"', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='GitHub data from the past year shows a remarkable increase in developer engagement with AI, particularly generative AI. In 2023, generative AI projects entered the top 10 most popular projects across the code hosting platform for the first time, with projects such as Stable Diffusion and AutoGPT pulling in thousands of first-time contributors.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content=\"Although generative AI tools were widely adopted in 2023, they continue to be plagued by the problem of hallucinations: plausible-sounding but incorrect responses to users' queries. This limitation has presented a roadblock to enterprise adoption, where hallucinations in business-critical or customer-facing scenarios could be catastrophic. Retrieval-augmented generation (RAG) has emerged as a technique for reducing hallucinations, with potentially profound implications for enterprise AI\", metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='\"It gives everyone easy, fairly democratized access, and it\\'s great for experimentation and exploration,\" Barrington said.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'})]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({'query': question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4bfa5e-8009-44c7-97d3-c3e77c6e8499",
   "metadata": {},
   "source": [
    "### Improving results\n",
    "\n",
    "We might be able to improve the results with a more specific prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "161477d6-95fd-415a-abf9-d0a0cbfbaf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"\n",
    "[INST]\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question, and only this context. \n",
    "If you don't know the answer, from the provided context, just say that you don't know. \n",
    "Do not attempt to define what acronyms stand for unless the definition was explicitly provided in the context.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\n",
    "[/INST]\n",
    "\"\"\"\n",
    "rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14b3bf78-2854-4f2f-b6b1-dc7c7987a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {'context': qa_chain, 'question': RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72105be6-1109-4baf-9622-1ca59621a5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRAG stands for Retrieval-Augmented Generation, which is a technique that combines text generation with information retrieval to improve the accuracy and relevance of AI-generated content. It allows large language models to access external information, reducing model size and increasing speed and cost efficiency. RAG has potential implications for enterprise AI adoption, particularly in creating more efficient and informative chatbots and virtual assistants.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5219356-eecf-4a23-bef2-9c40af875c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
