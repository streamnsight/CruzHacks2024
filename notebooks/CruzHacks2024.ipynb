{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7560cca5-0095-4fa1-8095-cc5e6b35a442",
   "metadata": {},
   "source": [
    "# Create AI apps with LLMs\n",
    "\n",
    "This notebook shows ways to experiment quickly with Large Language Models (LLMs) and \n",
    "Retrieval Augmented Generation (RAG) which can then be integrated into a UI / App\n",
    "\n",
    "For putting things together, Langchain is a very useful framework that integrates lots of different providers \n",
    "(LLM, vector databases, agents...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f93bd-f495-4f2e-b0a5-3fbcbca0134e",
   "metadata": {},
   "source": [
    "## RAG: Retrieval Augmented Generation\n",
    "\n",
    "LLMs only know about things they were trained on. They cannot know about everything, especially not \n",
    "about documents and data from private sources, or content published after the model was trained.\n",
    "\n",
    "To generate accurate answers querying specific content, the content needs to be passed to the LLM as part of the prompt.\n",
    "However, there is a major problem: despite having a large context window compared to other types of NLP models, the window \n",
    "is not unlimited. Typical window size is 1024, 4096, and up to 32000 tokens, which is often too small for even medium size documents.\n",
    "                                                                                                     \n",
    "The solution is to index the content, and provide only relevant context to the LLM.\n",
    "\n",
    "To do this, the content is chunked into small pieces of text, for each piece, an embedding vector of the sentence is created, and stored into\n",
    "a vector store. Upon querying the data, an embedding of the query is created, and the vector store is queried for similar content.\n",
    "The top N pieces of relevant content are retrieved and plugged into the prompt for the LLM to answer the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406b850-3d85-461c-823d-430bcdb8efb6",
   "metadata": {},
   "source": [
    "### Import some data to query\n",
    "\n",
    "In this example, we retrieve a recent article from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d635ec2-fde0-4fb3-b87e-8b379831d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# for other types of documents, use:\n",
    "# from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "# from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "# from langchain_community.document_loaders import JSONLoader\n",
    "# from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# see: https://python.langchain.com/docs/modules/data_connection/document_loaders/ for more info\n",
    "\n",
    "url = \"https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends\"\n",
    "loader = WebBaseLoader(url)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62079de0-a5ad-4f36-85d7-5514964761e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2542ea-be32-4576-b665-8fb4372b7f2b",
   "metadata": {},
   "source": [
    "### Chunk the article into smaller manageable pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a9a07-4ae2-4ae3-9b47-dda8f914dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=0, \n",
    "    length_function=len, \n",
    ")\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80820fa6-c0d0-43ce-8212-c28e90ea466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebaeb3-b449-46c9-b33c-61c1a74461f4",
   "metadata": {},
   "source": [
    "### Load an embedding model\n",
    "\n",
    "Note the same embedding model needs to be used to embed the pieces of text from the article, and later the query to be answered for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9b479-08fb-42de-8d92-ee2090a14ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# If you are on Mac M1/M2, enable the following environment variable\n",
    "# NotImplementedError: The operator 'aten::cumsum.out' is not currently implemented for the MPS device. \n",
    "# If you want this op to be added in priority during the prototype phase of this feature, \n",
    "# please comment on https://github.com/pytorch/pytorch/issues/77764. \n",
    "# As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` \n",
    "# to use the CPU as a fallback for this op. \n",
    "# WARNING: this will be slower than running natively on MPS.\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'mps'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embedding=HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c28a6a-087b-4168-9ce7-b698d51c6a84",
   "metadata": {},
   "source": [
    "### Ingest the data into the vector store\n",
    "\n",
    "Langchain vector store interface takes care of embedding each piece of text and store it in the DB.\n",
    "\n",
    "Here we use ChromaDB, a local vector database based on SQLLite. \n",
    "\n",
    "Note as we pass the split texts and the embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc44323-6bf9-4424-9ecb-b942582c8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "import time\n",
    "vectorstore = Chroma.from_documents(\n",
    "    collection_name=str(time.time()), # !!! if we re-run with the same collection name, we end up with duplicates in the DB!\n",
    "    documents=all_splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=\"./\"  # if you want to persist the DB locally, and not have to reindex each time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29baebd4-704d-44d1-a5a1-8ee9eacfae76",
   "metadata": {},
   "source": [
    "### Test the vector store retrieval on some question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61402235-6f93-48d6-ba3f-0923a24f2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is RAG?\"\n",
    "docs = vectorstore.similarity_search_with_score(question, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6422d-230e-48d9-b26d-4df748d7ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ead8e-96b8-45d1-8a19-bea5034e2313",
   "metadata": {},
   "source": [
    "### Setup the LLM\n",
    "\n",
    "### To use LlamaCpp\n",
    "\n",
    "LlamaCpp lets you run a model as a local LLM engine so everything runs locally. \n",
    "I use a small version of the model (13B params) quantized to 4bit that takes a lot less space than the full model.\n",
    "\n",
    "However, even then it requires a decent Nvidia GPU or a M1/M2 Mac. Alternatively, you can use a service like OpenAI.\n",
    "\n",
    "Note that on Mac, it requires to compile with special flags. See the README for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517296ca-24b3-4e1a-8cfd-7e65540b953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "model_file = '/Users/emmanuel/workspace/models/llms/llama-2-13b-chat.Q4_0.gguf'\n",
    "# downloaded from https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF\n",
    "# you may also try a smaller model:\n",
    "# https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_file,\n",
    "    n_ctx=4096, # context window\n",
    "    #verbose=True,\n",
    "    device='mps',\n",
    "    # model_kwargs={'device':'mps'},\n",
    "    n_gpu_layers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3718dfb4-1b96-4b56-98a6-585057a3d22e",
   "metadata": {},
   "source": [
    "## To use OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39231576-ecbd-40f6-ad71-934a3a40d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d2ad7-0bed-4d4d-80a8-0ea9288b680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAI\n",
    "# # llm = OpenAI(openai_api_key=\"...\")\n",
    "# llm = OpenAI(openai_api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1786e-cacc-4b29-a097-64ced8e2d851",
   "metadata": {},
   "source": [
    "## To use Ollama\n",
    "\n",
    "first setup Ollama (install the app, run it, this installs the command line\n",
    "\n",
    "Then run the server with \n",
    "```ollama run <model>```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51151df5-c633-4c5e-9520-e9dc945cef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.callbacks.manager import CallbackManager\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "# from langchain_community.llms import Ollama\n",
    "\n",
    "# llm = Ollama(\n",
    "#     model=\"llama2\",\n",
    "#     # callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90c654-0432-46b0-9fee-3c9c88213892",
   "metadata": {},
   "source": [
    "### Setup a retrieval chain\n",
    "\n",
    "Another useful langchain abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f9f1a-ab44-4e56-a1b0-907c12e134d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectorstore.as_retriever(search_kwargs={'k':2}, k=2, search_type=\"mmr\"),\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9be088-09fd-4d28-9032-34110a1585a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain({'query': question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4bfa5e-8009-44c7-97d3-c3e77c6e8499",
   "metadata": {},
   "source": [
    "### Improving results\n",
    "\n",
    "We might be able to improve the results with a more specific prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161477d6-95fd-415a-abf9-d0a0cbfbaf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"\n",
    "[INST]\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question, and only this context. \n",
    "If you don't know the answer, from the provided context, just say that you don't know. \n",
    "Do not attempt to define what acronyms stand for unless the definition was explicitly provided in the context.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\n",
    "[/INST]\n",
    "\"\"\"\n",
    "rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3bf78-2854-4f2f-b6b1-dc7c7987a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {'context': qa_chain, 'question': RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72105be6-1109-4baf-9622-1ca59621a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea9a9e-f97b-4c0a-a6a8-63a387a54271",
   "metadata": {},
   "source": [
    "### More tuning?\n",
    "\n",
    "Sometimes, it's not enough, and we need ot revisit our strategy from the start\n",
    "- Chunking: how is the document split up? Here we have text from the web. We know the article is split up into several independent paragraphs.\n",
    "it might be useful to have larger chunks that include a whole paragraph, and if that is too much to fit in the context window, we can reduce the numebr of matches to provide, since with larger chunks, other paragraphs may not be relevant anyway.,\n",
    "\n",
    "Setting chunk size to 5000 in the chunking phase helps improve results.\n",
    "\n",
    "- Search method: we use MMR (Maximal Marginal Relevance) already. This is an option that may reduce redundant chunks.\n",
    "\n",
    "- LLM model: choosing a model trained more closely to the task is always a good strategy. If you're dealing with code, use an instruct model trained on code.\n",
    "\n",
    "- Prompt: prompt engineering is the best thing before requiring to fine tune the model. More precise instructions and directions in prompts help weed out bad answers.\n",
    "\n",
    "- Fine Tuning: if nothing helps, the last resort might be to require to fine tune a model. That is expensive and time consuming, and to be considered with all the other options above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f74d1-a7d4-400d-94fd-5274e71fe84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
