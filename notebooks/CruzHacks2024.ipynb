{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7560cca5-0095-4fa1-8095-cc5e6b35a442",
   "metadata": {},
   "source": [
    "# Create AI apps with LLMs\n",
    "\n",
    "This notebook shows ways to experiment quickly with Large Language Models (LLMs) and \n",
    "Retrieval Augmented Generation (RAG) which can then be integrated into a UI / App\n",
    "\n",
    "For putting things together, Langchain is a very useful framework that integrates lots of different providers \n",
    "(LLM, vector databases, agents...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f93bd-f495-4f2e-b0a5-3fbcbca0134e",
   "metadata": {},
   "source": [
    "## RAG: Retrieval Augmented Generation\n",
    "\n",
    "LLMs only know about things they were trained on. They cannot know about everything, especially not \n",
    "about documents and data from private sources, or content published after the model was trained.\n",
    "\n",
    "To generate accurate answers querying specific content, the content needs to be passed to the LLM as part of the prompt.\n",
    "However, there is a major problem: despite having a large context window compared to other types of NLP models, the window \n",
    "is not unlimited. Typical window size is 1024, 4096, and up to 32000 tokens, which is often too small for even medium size documents.\n",
    "                                                                                                     \n",
    "The solution is to index the content, and provide only relevant context to the LLM.\n",
    "\n",
    "To do this, the content is chunked into small pieces of text, for each piece, an embedding vector of the sentence is created, and stored into\n",
    "a vector store. Upon querying the data, an embedding of the query is created, and the vector store is queried for similar content.\n",
    "The top N pieces of relevant content are retrieved and plugged into the prompt for the LLM to answer the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406b850-3d85-461c-823d-430bcdb8efb6",
   "metadata": {},
   "source": [
    "### Import some data to query\n",
    "\n",
    "In this example, we retrieve a recent article from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d635ec2-fde0-4fb3-b87e-8b379831d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# for other types of documents, use:\n",
    "# from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "# from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "# from langchain_community.document_loaders import JSONLoader\n",
    "# from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# see: https://python.langchain.com/docs/modules/data_connection/document_loaders/ for more info\n",
    "\n",
    "url = \"https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends\"\n",
    "loader = WebBaseLoader(url)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2542ea-be32-4576-b665-8fb4372b7f2b",
   "metadata": {},
   "source": [
    "### Chunk the article into smaller manageable pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e7a9a07-4ae2-4ae3-9b47-dda8f914dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, \n",
    "    chunk_overlap=10, \n",
    "    length_function=len, \n",
    ")\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80820fa6-c0d0-43ce-8212-c28e90ea466b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebaeb3-b449-46c9-b33c-61c1a74461f4",
   "metadata": {},
   "source": [
    "### Load an embedding model\n",
    "\n",
    "Note the same embedding model needs to be used to embed the pieces of text from the article, and later the query to be answered for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "54a9b479-08fb-42de-8d92-ee2090a14ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# If you are on Mac M1/M2, enable the following environment variable\n",
    "# NotImplementedError: The operator 'aten::cumsum.out' is not currently implemented for the MPS device. \n",
    "# If you want this op to be added in priority during the prototype phase of this feature, \n",
    "# please comment on https://github.com/pytorch/pytorch/issues/77764. \n",
    "# As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` \n",
    "# to use the CPU as a fallback for this op. \n",
    "# WARNING: this will be slower than running natively on MPS.\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'mps'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embedding=HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c28a6a-087b-4168-9ce7-b698d51c6a84",
   "metadata": {},
   "source": [
    "### Ingest the data into the vector store\n",
    "\n",
    "Langchain vector store interface takes care of embedding each piece of text and store it in the DB.\n",
    "\n",
    "Here we use ChromaDB, a local vector database based on SQLLite. \n",
    "\n",
    "Note as we pass the split texts and the embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0cc44323-6bf9-4424-9ecb-b942582c8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embedding,\n",
    "    # persist_directory=\"./\"  # if you want to persist the DB locally, and not have to reindex each time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29baebd4-704d-44d1-a5a1-8ee9eacfae76",
   "metadata": {},
   "source": [
    "### Test the vector store retrieval on some question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61402235-6f93-48d6-ba3f-0923a24f2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is RAG?\"\n",
    "docs = vectorstore.similarity_search_with_score(question, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55a6422d-230e-48d9-b26d-4df748d7ff4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='\"You can use RAG to go gather a ton of unstructured information, documents, etc., [and] feed it into a model without having to fine-tune or custom-train a model,\" Barrington said.\\nThese benefits are particularly enticing for enterprise applications where up-to-date factual knowledge is crucial. For example, businesses can use RAG with foundation models to create more efficient and informative chatbots and virtual assistants.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  0.8618921637535095),\n",
       " (Document(page_content='\"You can use RAG to go gather a ton of unstructured information, documents, etc., [and] feed it', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  0.9121479392051697),\n",
       " (Document(page_content='factual knowledge is crucial. For example, businesses can use RAG with foundation models to create', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  1.208876132965088)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ead8e-96b8-45d1-8a19-bea5034e2313",
   "metadata": {},
   "source": [
    "### Setup the LLM\n",
    "\n",
    "Below I use LlamaCpp as a local LLM engine so everything runs locally. \n",
    "I use a small version of the model (7B params) quantized to 4bit that takes a lot less space than the full model.\n",
    "\n",
    "However, even then it requires a decent Nvidia GPU or a M1/M2 Mac. Alternatively, you can use a service like OpenAI.\n",
    "\n",
    "Note that on Mac, it requires to compile with special flags. See the README for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "517296ca-24b3-4e1a-8cfd-7e65540b953a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuel/workspace/code/me/CruzHacks/CruzHacks2024/.venv/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! device is not default parameter.\n",
      "                device was transferred to model_kwargs.\n",
      "                Please confirm that device is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /Users/emmanuel/workspace/models/llms/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   170.20 MiB, ( 2506.06 / 49152.00)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7023.90 MiB\n",
      "llm_load_tensors:      Metal buffer size =   170.20 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Max\n",
      "ggml_metal_init: picking default device: Apple M2 Max\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/emmanuel/workspace/code/me/CruzHacks/CruzHacks2024/.venv/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 51539.61 MB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3200.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 2506.08 / 49152.00)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    85.48 MiB, ( 2591.55 / 49152.00)\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "llama_new_context_with_model:      Metal compute buffer size =    85.47 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     5.59 MiB\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "model_file = '/Users/emmanuel/workspace/models/llms/llama-2-13b-chat.Q4_0.gguf'\n",
    "# downloaded from https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF\n",
    "# you may also try a smaller model:\n",
    "# https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_file,\n",
    "    n_ctx=4096, # context window\n",
    "    #verbose=True,\n",
    "    device='mps',\n",
    "    # model_kwargs={'device':'mps'},\n",
    "    n_gpu_layers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3718dfb4-1b96-4b56-98a6-585057a3d22e",
   "metadata": {},
   "source": [
    "## To use OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39231576-ecbd-40f6-ad71-934a3a40d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee4d2ad7-0bed-4d4d-80a8-0ea9288b680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_openai import OpenAI\n",
    "#llm = OpenAI(openai_api_key=\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90c654-0432-46b0-9fee-3c9c88213892",
   "metadata": {},
   "source": [
    "### Setup a retrieval chain\n",
    "\n",
    "Another useful langchain abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "513f9f1a-ab44-4e56-a1b0-907c12e134d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectorstore.as_retriever(search_kwargs={'k':10}),\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e9be088-09fd-4d28-9032-34110a1585a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     791.79 ms\n",
      "llama_print_timings:      sample time =       3.77 ms /    36 runs   (    0.10 ms per token,  9559.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25280.58 ms /   366 tokens (   69.07 ms per token,    14.48 tokens per second)\n",
      "llama_print_timings:        eval time =    3600.72 ms /    35 runs   (  102.88 ms per token,     9.72 tokens per second)\n",
      "llama_print_timings:       total time =   28985.01 ms /   401 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is RAG?',\n",
       " 'result': ' RAG stands for \"Read, Ask, Generate\" and is a technique for reducing hallucinations in language models by blending text generation with information retrieval.',\n",
       " 'source_documents': [Document(page_content='\"You can use RAG to go gather a ton of unstructured information, documents, etc., [and] feed it into a model without having to fine-tune or custom-train a model,\" Barrington said.\\nThese benefits are particularly enticing for enterprise applications where up-to-date factual knowledge is crucial. For example, businesses can use RAG with foundation models to create more efficient and informative chatbots and virtual assistants.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='\"You can use RAG to go gather a ton of unstructured information, documents, etc., [and] feed it', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='factual knowledge is crucial. For example, businesses can use RAG with foundation models to create', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='RAG blends text generation with information retrieval to enhance the accuracy and relevance of', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='RAG blends text generation with information retrieval to enhance the accuracy and relevance of AI-generated content. It enables LLMs to access external information, helping them produce more accurate and contextually aware responses. Bypassing the need to store all knowledge directly in the LLM also reduces model size, which increases speed and lowers costs.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='generation (RAG) has emerged as a technique for reducing hallucinations, with potentially profound', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='use of it.\"', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='in a real product setting,\" Luke said.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='to contribute to and build on existing code.', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'}),\n",
       "  Document(page_content='in this space.\"', metadata={'description': 'Discover the top 10 machine learning and AI trends for 2024 that are shaping technology and business, including multimodal, open source and customization.', 'language': 'en', 'source': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'title': '10 top AI and machine learning trends for 2024 | TechTarget'})]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({'query': question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4bfa5e-8009-44c7-97d3-c3e77c6e8499",
   "metadata": {},
   "source": [
    "### Improving results\n",
    "\n",
    "We might be able to improve the results with a more specific prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "161477d6-95fd-415a-abf9-d0a0cbfbaf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Do not attempt to define what acronyms stands for unless the definition was explicitly provided in the context.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\n",
    "\"\"\"\n",
    "rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14b3bf78-2854-4f2f-b6b1-dc7c7987a2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {'context': qa_chain, 'question': RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "72105be6-1109-4baf-9622-1ca59621a5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     791.79 ms\n",
      "llama_print_timings:      sample time =       6.53 ms /    73 runs   (    0.09 ms per token, 11186.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    6984.72 ms /    73 runs   (   95.68 ms per token,    10.45 tokens per second)\n",
      "llama_print_timings:       total time =    7085.44 ms /    74 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     791.79 ms\n",
      "llama_print_timings:      sample time =       6.85 ms /    74 runs   (    0.09 ms per token, 10806.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =  131379.85 ms /  1704 tokens (   77.10 ms per token,    12.97 tokens per second)\n",
      "llama_print_timings:        eval time =    9563.49 ms /    73 runs   (  131.01 ms per token,     7.63 tokens per second)\n",
      "llama_print_timings:       total time =  141284.02 ms /  1777 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RAG stands for Relevance-Aware Generation, a technique used to reduce hallucinations in AI-generated content by blending text generation with information retrieval. It enhances the accuracy and relevance of AI-generated content, enables LLMs to access external information, and reduces model size, increasing speed and lowering costs.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5219356-eecf-4a23-bef2-9c40af875c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
